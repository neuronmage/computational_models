{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RIC3-CPDM CASANDRE Parameter Estimation**\n",
    "\n",
    "#### Python Implementation of CASANDRE sortData.m, fitData.m, saveData.m, getLlhChoice.m (MATLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - Proof of equivalent mathematical CASANDRE and NLL outcomes:\n",
    "\n",
    "![casandre-matlab](proof/casandre-matlab1.PNG) \n",
    "![casandre-python](proof/casandre-py1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLL-matlab](proof/neg_ll-matlab2.PNG)\n",
    "![NLL-python](proof/neg_ll-py2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================\n",
    "Mandy Renfro (2024)\n",
    "===================\n",
    "\"\"\"\n",
    "\n",
    "from glob import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import exp, linspace, log, sqrt, sum\n",
    "np.seterr(all = \"ignore\")\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erfcinv\n",
    "normcdf = stats.norm.cdf\n",
    "#import seaborn as sns\n",
    "#sns.set_theme(style=\"white\", palette=\"muted\")\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import time\n",
    "\n",
    "base_proj_dir = \"Z:/data/RIC\" ## base project directory\n",
    "data_dir      = \"Z:/data/RIC/sourcedata/RIC3\" ## directory containing data\n",
    "save_proj_dir = os.path.join(base_proj_dir, \"derivatives/RIC3/parameter_estimation/cpdm\") ## output directory\n",
    "## CASANDRE parameters\n",
    "cruns         = 10   ## CASANDRE runs per Ss\n",
    "sample_rate   = 30   ## higher values produce slower, more precise estimates\n",
    "delta         = 5    ## SDs below and above mean, computes confidence variable distributions\n",
    "noise_sens    = 1    ## if sensory noise == 1, distributions of decision variable and confidence variable can be compared directly\n",
    "## CASANDRE options\n",
    "options       = dict(maxiter = 250, disp = False) ## primary factor influencing runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casandre_fit(guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit, stim_vals):\n",
    "    \"\"\" Compute llh of each response alternative\n",
    "        Step 1 - sample decision variable denominator in steps of constant cumulative density.\n",
    "        Step 2 - compute choice distribution under each scaled sensory distribution.\n",
    "        Step 3 - average across all scaled sensory distributions to get likelihood functions.\n",
    "        **Linear transformation of normal variable is itself normal variable.\n",
    "        **Inverse of denominator used here to work with products instead of ratios.\n",
    "        INPUT\n",
    "        - params:    CASANDRE parameters [guessRate, stimSens, stimCrit, uncMeta, confCrit]\n",
    "        - stim_vals: stimulus conditions in units of stimulus magnitude\n",
    "        OUTPUT\n",
    "        - choice_ll: likelihood of each choice (2 x # confidence levels x N stim_vals) \n",
    "    \"\"\"\n",
    "    sens_mean           = stim_vals * stim_sens ## ??? piece of numerator (SNR) with unknown specific name\n",
    "    sens_crit           = stim_crit * stim_sens ## second piece of numerator (SNR)\n",
    "    x                   = [-conf_crit, 0, conf_crit] ## bounds splitting conf space into four quadrants (determined by conf levs)\n",
    "    guess_rate_weight   = guess_rate / (len(x) + 1) ## weigh by number of regions in confidence space\n",
    "    choice_llh          = np.zeros((len(x) + 1, len(stim_vals))) ## setting up short vector of 4 x conditions (or trials?)\n",
    "    sample_rate_div     = 0.5 / sample_rate ## optimization\n",
    "    meta_power          = meta_uncert**2 ## optimization\n",
    "    mu_log_n            = log((noise_sens**2) / sqrt(meta_power + noise_sens**2)) ## mean of prob space accounting for metauncertainty\n",
    "    sigma_log_n         = sqrt(log((meta_power) / (noise_sens**2) + 1)) ## SD of prob space accounting for metauncertainty\n",
    "    ## inverse of sensitivity to decision reliability (denominator of SNR)\n",
    "    dv_Den_x            = 1 / logninv(linspace(sample_rate_div, 1 - sample_rate_div, sample_rate),  mu_log_n, sigma_log_n).reshape(-1, 1)\n",
    "    sigma               = dv_Den_x * noise_sens ## SD of confidence space\n",
    "    \n",
    "    for stim_idx in range(len(stim_vals)): ## iterates through trials for given contrast\n",
    "        ## incorporated 1/denominator above loop to improve computational efficiency\n",
    "        mu = dv_Den_x * (sens_mean[stim_idx] - sens_crit) ## 1/denominator * numerator (SNR)\n",
    "        p  = normcdf(repeat_matrix_function_2D(x, sample_rate, 1), ## cumsum at each bound of x\n",
    "                    repeat_matrix_function_2D(mu, 1, len(x)), \n",
    "                    repeat_matrix_function_2D(sigma, 1, len(x)))\n",
    "        ratio_dist_p = np.mean(p, axis = 0) ## gets avg cumsum up to each x bound\n",
    "        #### Get probabilities of each quad and weight them by guessrate to use as likelihoods\n",
    "        ## delta cumsum of mid-quadrants to get specific probabilities\n",
    "        choice_llh[1:-1, stim_idx] = guess_rate_weight + (1 - guess_rate) * (ratio_dist_p[1:] - ratio_dist_p[:-1])\n",
    "        ## cumsum of first quadrant\n",
    "        choice_llh[   0, stim_idx] = guess_rate_weight + (1 - guess_rate) * (ratio_dist_p[0])\n",
    "        ## cumsum of last quadrant\n",
    "        choice_llh[  -1, stim_idx] = guess_rate_weight + (1 - guess_rate) * (1 - ratio_dist_p[-1])\n",
    "    return choice_llh\n",
    "\n",
    "\n",
    "def _logninv(p):\n",
    "    \"\"\" Returns log inverse only when mu = 0 and sigma = 1.\n",
    "        INPUT\n",
    "        - p: one or more cumsums of log normal distribution\n",
    "        OUTPUT\n",
    "        - log inverse\n",
    "    \"\"\"\n",
    "    return exp(-sqrt(2) * erfcinv(2 * p))\n",
    "\n",
    "\n",
    "def logninv(p, mu, sigma):\n",
    "    \"\"\" Returns log inverse not only for mu = 0 and sigma = 1.\n",
    "        If you want to get the log normal inverse of a cumsum (p),\n",
    "        you can use a definitional trick to compute it with a function that \n",
    "        only provides the inverse with mu = 0 and sigma = 1 because:\n",
    "        --> log(logninv(p, mu, sigma)) == mu + sigma * log(logninv(p, 0, 1))\n",
    "        the inverse of the natural log is exp()\n",
    "        --> exp(log(logninv(p, mu, sigma))) == logninv(p, mu, sigma)\n",
    "        INPUT\n",
    "        - p: one or more cumsums of log normal distribution\n",
    "        - mu: avg of distribution\n",
    "        - sigma: SD of distribution\n",
    "        OUTPUT\n",
    "        - log inverse\n",
    "    \"\"\"\n",
    "    return exp(mu + sigma * log(_logninv(p)))\n",
    "\n",
    "\n",
    "def neg_ll(params_vec, stim_vals, choices, all_run_idx, all_contrast_idx, neg_llh_size):\n",
    "    \"\"\" Returns negative log likelihood for entire parameter vector.\n",
    "        INPUT\n",
    "        - param_vec: list of all modeling parameters\n",
    "        - stim_vals: list of stimulus orientations grouped by condition and contrast\n",
    "        - choices: list of Ss choice grouped by condition and contrast\n",
    "        - all_run_idx: np.where results identifying locations of trials within each run\n",
    "        - all_contrast_idx: np.where results identifying locations of trials within each contrast\n",
    "        - neg_llh_size: precounted number of summed log likelihoods\n",
    "        OUTPUT\n",
    "        - returns final sum of negative log likelihood for model parameters across contrasts \n",
    "        and runs (called up to 250 times per crun)\n",
    "    \"\"\"\n",
    "    max_contrast_len = 0 ## optimization\n",
    "    for key in all_contrast_idx: ## optimization\n",
    "        max_contrast_len = max(max_contrast_len, len(all_contrast_idx[key])) ## optimization\n",
    "    max_contrast_len1 = 1 + max_contrast_len ## optimization\n",
    "    max_contrast_len2 = max_contrast_len1 + max_contrast_len ## optimization\n",
    "    max_contrast_len3 = max_contrast_len2 + len(all_run_idx) ## optimization\n",
    "    neg_llh           = np.zeros(neg_llh_size) ## empt array to hold negative LLHs\n",
    "    counter           = 0 ## keep track of where to store each LLH\n",
    "    for idx_run in range(len(all_run_idx)): ## each condition run\n",
    "        run_stim_vals = stim_vals[all_run_idx[idx_run]] ## stim vals for current run\n",
    "        run_choices   = choices[all_run_idx[idx_run]] ## choices for current run\n",
    "        for idx_contrast in range(len(all_contrast_idx[idx_run])): ## each contrast for current run\n",
    "            ## CASANDRE parameters\n",
    "            guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit = params_vec[(np.array([0, \n",
    "                                                                                    1 + idx_contrast,\n",
    "                                                                                    max_contrast_len1 + idx_contrast,  \n",
    "                                                                                    max_contrast_len2 + idx_run, \n",
    "                                                                                    max_contrast_len3 + idx_run]))]\n",
    "            unique_stim_choices = run_choices[all_contrast_idx[idx_run][idx_contrast]] ## corresponding choices for each stimulus\n",
    "            ## executing CASANDRE\n",
    "            choice_llh          = casandre_fit(guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit, \n",
    "                                                run_stim_vals[all_contrast_idx[idx_run][idx_contrast]])\n",
    "            ## summed negative LLHs of choices for the current contrast and run\n",
    "            neg_llh[counter]    = -sum(unique_stim_choices * log(choice_llh.T)) \n",
    "            counter += 1 ## increment index in neg_llh\n",
    "    return np.nansum(neg_llh)\n",
    "\n",
    "\n",
    "def nll_trials(params_vec, stim_vals, choices, all_run_idx, all_contrast_idx, neg_llh_size): #### NOT READY\n",
    "    \"\"\" Returns negative log likelihood for entire parameter vector.\n",
    "        INPUT\n",
    "        - param_vec: list of all modeling parameters\n",
    "        - stim_vals: list of stimulus orientations grouped by condition and contrast\n",
    "        - choices: list of Ss choice grouped by condition and contrast\n",
    "        - all_run_idx: np.where results identifying locations of trials within each run\n",
    "        - all_contrast_idx: np.where results identifying locations of trials within each contrast\n",
    "        - neg_llh_size: precounted number of summed log likelihoods\n",
    "        OUTPUT\n",
    "        - returns log likelihood for each tilt/confidence choice combo [HCLT, LCLT, LCRT, HCRT]\n",
    "    \"\"\"\n",
    "    max_contrast_len = 0 ## optimization\n",
    "    for key in all_contrast_idx: ## optimization\n",
    "        max_contrast_len = max(max_contrast_len, len(all_contrast_idx[key])) ## optimization\n",
    "    max_contrast_len1 = 1 + max_contrast_len ## optimization\n",
    "    max_contrast_len2 = max_contrast_len1 + max_contrast_len ## optimization\n",
    "    max_contrast_len3 = max_contrast_len2 + len(all_run_idx) ## optimization\n",
    "    neg_llh           = [] ## empty array to hold negative LLHs\n",
    "    counter           = 0 ## keep track of where to store each LLH\n",
    "    for idx_run in range(len(all_run_idx)): ## each condition [LVNR, LVLR, LVHR]\n",
    "        run_stim_vals = stim_vals[all_run_idx[idx_run]] ## stim vals for current run\n",
    "        run_choices   = choices[all_run_idx[idx_run]] ## choices for current run\n",
    "        for idx_contrast in range(len(all_contrast_idx[idx_run])): ## each contrast for current run\n",
    "            ## CASANDRE parameters\n",
    "            guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit = params_vec[(np.array([0, \n",
    "                                                                                    1 + idx_contrast,\n",
    "                                                                                    max_contrast_len1 + idx_contrast,  \n",
    "                                                                                    max_contrast_len2 + idx_run, \n",
    "                                                                                    max_contrast_len3 + idx_run]))]\n",
    "            unique_stim_choices = run_choices[all_contrast_idx[idx_run][idx_contrast]] ## corresponding choices for each stimulus\n",
    "            ## executing CASANDRE\n",
    "            choice_llh          = casandre_fit(guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit, \n",
    "                                                run_stim_vals[all_contrast_idx[idx_run][idx_contrast]])\n",
    "            ## summed negative LLHs of choices for the current contrast and run\n",
    "            neg_llh.append(-log(choice_llh.T)) \n",
    "            counter += 1 ## increment index in neg_llh\n",
    "    return np.nansum(neg_llh)\n",
    "\n",
    "\n",
    "def random_bounded(bounds):\n",
    "    \"\"\" Generate random value within parameter range.\n",
    "        INPUT\n",
    "        - bounds: tuple containing minimum and maximum values of parameter range.\n",
    "        OUTPUT\n",
    "        - returns random value within bounded range.\n",
    "    \"\"\"\n",
    "    return np.random.random() * (bounds[1] - bounds[0]) + bounds[0]\n",
    "\n",
    "\n",
    "def repeat_matrix_function_2D(arr, x, y = 1):\n",
    "    \"\"\" Python implementation of MATLAB's repmat().\n",
    "        INPUT\n",
    "        - arr: array to be copied\n",
    "        - x:   number of rows\n",
    "        - y:   number of columns\n",
    "        OUTPUT\n",
    "        - new_arr: new array \n",
    "    \"\"\"\n",
    "    new_arr = arr.copy()\n",
    "    for i in range(1, x):\n",
    "        new_arr = np.vstack((new_arr, arr))\n",
    "    arr_h   = new_arr.copy()\n",
    "    for i in range(1, y):\n",
    "        new_arr = np.hstack((new_arr, arr_h))\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_rate_bounds  = (   0, 0.075)\n",
    "stim_sens_bounds   = (0.01,    10)\n",
    "stim_crit_bounds   = (  -3,     3)\n",
    "meta_uncert_bounds = ( 0.1,     3)\n",
    "conf_crit_bounds   = (0.01,   5.1)\n",
    "subs               = [] ## list of Ss IDs\n",
    "all_sub_params     = [] ## list containing all Ss extimated CASANDRE parameters\n",
    "\n",
    "files = sorted(glob(os.path.join(data_dir, \"24_IDM_*.csv\"))) ## grab all Ss datafiles\n",
    "for curr_file in files: ## iterate through globbed files and save Ss ID to a list\n",
    "    sub_id = os.path.basename(curr_file)[7:11] ## grab first 5 indices of filename string\n",
    "    if not sub_id in subs: ## check if already in list\n",
    "        subs.append(sub_id) ## if not, append new Ss ID to list\n",
    "\n",
    "skip_num = 0 ## counter used to allow continuation of writing CASANDRE param output across Ss if something breaks\n",
    "if os.path.exists(join(save_proj_dir, \"ric3-cpdm_best-params-est.csv\")): ## checks for previous best params file\n",
    "    existing_df          = pd.read_csv(join(save_proj_dir, \"ric3-cpdm_best-params-est.csv\")) ## make new Ss save directory\n",
    "    existing_df[\"subID\"] = [val[2:-1] for val in existing_df[\"subID\"]] ## gets subID accounting for padding\n",
    "    skip_num             = len(existing_df[\"subID\"].values) ## current number of completed Ss\n",
    "    all_sub_params       = existing_df.values.tolist() ## new output df with current Ss\n",
    "\n",
    "for idx, sub in enumerate(subs): ## iterate through Ss IDs\n",
    "    if idx + 1 <= skip_num: ## checks if current Ss has already been run\n",
    "        continue ## go to next Ss\n",
    "    sub_files = sorted(glob(os.path.join(data_dir, \"24_IDM_{0}.csv\".format(sub)))) ## grab all IDM task csvs\n",
    "    sub_cols  = [\"run_dims\", \"orient\", \"contrast\", \"acc\", \"conf\", \"choice\"] ## trial/Ss resp elements\n",
    "    sub_df    = pd.DataFrame(columns = sub_cols) ## Ss-specific dataframe w/ preset columns\n",
    "    raw_df    = pd.read_csv(sub_files[0]) ## open current data file\n",
    "    df        = raw_df.loc[(raw_df[\"cpdm_trial_type\"] == \"task\") & (raw_df[\"cpdm_trial_resp.keys\"].notnull())] ## only CPDM task trials w/ responses\n",
    "    sub_df[\"run_dims\"] = df[\"cpdm_run_dimension\"] ## dimensions for current trial (volatility/risk levels = number/difficulty of orientations/contrasts)\n",
    "    sub_df[\"orient\"]   = df[\"cpdm_gabor_orient\"] ## orientation of stimulus (gabor patch)\n",
    "    sub_df[\"contrast\"] = df[\"cpdm_gabor_contrast\"] ## contrast of stimulus (gabor patch)\n",
    "    sub_df[\"acc\"]      = df[\"cpdm_acc\"] ## Ss accuracy\n",
    "    sub_df[\"conf\"]     = df[\"cpdm_conf\"] ## Ss confidence in perceptual accuracy\n",
    "    sub_df[\"choice\"]   = df[\"cpdm_trial_resp.keys\"] ## trial choice (q=high conf/left tilt; a=low conf/left tilt; p=high conf/right tilt; l=low conf/right tilt)\n",
    "    choice_dict        = {'q':-2, 'a':-1, 'l':1, 'p':2} ## remap CPDM choice responses\n",
    "    sub_df[\"choice\"]   = df[\"cpdm_trial_resp.keys\"].replace(choice_dict) ## replace old choices with remapped values\n",
    "    \n",
    "    stim_vals     = sub_df[\"orient\"].values ## all stimulus orientations for current sub\n",
    "    contrast_vals = sub_df[\"contrast\"].values ## all stimulus contrast values for current sub\n",
    "    choices       = sub_df[\"choice\"].values ## all choice decisions for current sub\n",
    "    math_choices  = np.zeros((len(choices), len(choice_dict.keys()))) ## zeros matrix to contain choice arrays\n",
    "    math_choices[np.where(choices ==  2)] = np.array([0, 0, 0, 1]) ## choice array for high conf/right tilt\n",
    "    math_choices[np.where(choices ==  1)] = np.array([0, 0, 1, 0]) ## choice array for low conf/right tilt\n",
    "    math_choices[np.where(choices == -1)] = np.array([0, 1, 0, 0]) ## choice array for low conf/left tilt\n",
    "    math_choices[np.where(choices == -2)] = np.array([1, 0, 0, 0]) ## choice array for high conf/left tilt\n",
    "    \n",
    "    all_run_idx      = [] ## list of list of trial indices by condition [low_vol_no_risk, low_vol_low_risk, low_vol_high_risk]\n",
    "    all_contrast_idx = {} ## dictionary containing tuples of arrays for indices of contrasts organized by run (keys)\n",
    "    nll_counter      = 0 ## number of times LL is placed in NLL array\n",
    "    for idx_run, run_label in enumerate(sorted(sub_df[\"run_dims\"].unique())): ## iterate through conditions (runs)\n",
    "        trial_idx  = np.where(sub_df[\"run_dims\"].values == run_label) ## current run indices\n",
    "        all_run_idx.append(trial_idx) ## append current run index list to all_run_idx\n",
    "        run_contrast_vals = contrast_vals[trial_idx] ## contrast values for current run indices\n",
    "        all_contrast_idx[idx_run] = [] ## create key and empty value array for current run\n",
    "        for unique_contrast in sorted(np.unique(run_contrast_vals)): ## iterate through unique stimulus contrast values\n",
    "            unique_contrast_idx = np.where(run_contrast_vals == unique_contrast) ## indices for current contrast within current run\n",
    "            all_contrast_idx[idx_run].append(unique_contrast_idx) ## add unique contrast indices to all_contrast_idx under appropriate key\n",
    "            nll_counter += len(unique_contrast_idx) ## increment nll_counter by number of current contrast trials\n",
    "\n",
    "    best_param_est = None ## set empty best params variable\n",
    "    run_labels     = dict(low_vol_no_risk = \"HVNR\", low_vol_low_risk = \"HVLR\", low_vol_high_risk = \"LVHR\") ## output column identifiers\n",
    "    for crun in range(cruns): ## CASANDRE runs\n",
    "        crun_start_time = time.time()\n",
    "        ## prints current Ss number and current CASANDRE run\n",
    "        print(\"-------------> Currently running sub-{0}\".format(sub), \"on CASANDRE Run #\", \"{0:02d}\".format(crun+1), end = \"\")\n",
    "        search_start = [random_bounded(guess_rate_bounds)] ## starting point for current CASANDRE run\n",
    "        bounds       = [guess_rate_bounds] ## list containing parameter bounds (starting points for each param space added below)\n",
    "        column_names = [\"subID\", \"Guess Rate\"] ## parameter output column names\n",
    "        for curr_contrast in sorted(np.unique(contrast_vals)): ## for each level of contrast\n",
    "            search_start.append(random_bounded(stim_sens_bounds)) ## random stimulus sensitivity starting point (bounded)\n",
    "            bounds.append(stim_sens_bounds) ## stimulus sensitivity bounds\n",
    "            column_names.append(\"Stimulus Sensitivity {}\".format(curr_contrast)) ## stimulus sensitivity column label\n",
    "        for curr_contrast in sorted(np.unique(contrast_vals)): ## for each level of contrast\n",
    "            search_start.append(random_bounded(stim_crit_bounds)) ## random stimulus criterion starting point (bounded)\n",
    "            bounds.append(stim_crit_bounds) ## stimulus criterion bounds\n",
    "            column_names.append(\"Stimulus Criterion {}\".format(curr_contrast)) ## stimulus criterion column label\n",
    "        for curr_run in sorted(sub_df[\"run_dims\"].unique()): ## for each conditional run\n",
    "            search_start.append(random_bounded(meta_uncert_bounds)) ## random meta uncertainty starting point (bounded)\n",
    "            bounds.append(meta_uncert_bounds) ## meta uncertainty bounds\n",
    "            column_names.append(\"Meta Uncertainty {}\".format(run_labels[curr_run])) ## meta uncertainly column label\n",
    "        for curr_run in sorted(sub_df[\"run_dims\"].unique()): ## for each conditional run\n",
    "            search_start.append(random_bounded(conf_crit_bounds)) ## random confidence criterion starting point (bounded)\n",
    "            bounds.append(conf_crit_bounds) ## confidence criterion bounds\n",
    "            column_names.append(\"Confidence Criterion {}\".format(run_labels[curr_run])) ## confidence criterion column label\n",
    "        search_start = np.array(search_start) ## convert to numpy array\n",
    "\n",
    "        estimate = minimize(neg_ll, \n",
    "                            x0      = search_start, ## optimizer starting point\n",
    "                            method  = \"L-BFGS-B\", ## descent direction by preconditioning gradient with curvature info (b = boundary limits)\n",
    "                            bounds  = bounds, ## dimensional bounds for each parameter\n",
    "                            args    = (stim_vals, math_choices, all_run_idx, all_contrast_idx, nll_counter), \n",
    "                            options = options) ## number of maxiters\n",
    "        crun_end_time = time.time()\n",
    "        ## prints number of iters required for convergence of --previous-- CASANDRE run\n",
    "        print(\" | Last crun iters: {0:03d} | Last crun time: {1:03d}\".format(estimate.nit, int(crun_end_time-crun_start_time)), end = \"\\r\") \n",
    "        best_param_est = np.hstack((sub, estimate.x)) ## horizonally add ID to subject row containing best parameter estimates for each crun\n",
    "    all_sub_params.append(best_param_est) ## add best parameter estimate for current Ss to list of all Ss\n",
    "    ## re-saving estimated parameters csv after each Ss in case of failure\n",
    "    estimate_df          = pd.DataFrame(np.array(all_sub_params), columns = column_names) ## convert all Ss into dataframe\n",
    "    estimate_df[\"subID\"] = estimate_df[\"subID\"].apply('=\"{}\"'.format) ## force zero padding to output csv\n",
    "    estimate_df.to_csv(join(save_proj_dir, \"ric3-cpdm_best-params-est.csv\"), index = False) ## save csv\n",
    "\n",
    "## final save of complete CASANDRE parameters csv with CRDM alpha parameter\n",
    "crdm_params_file          = os.path.join(base_proj_dir, \"derivatives/RIC3/parameter_estimation/crdm/softmax/ric3-crdm-sm_modelfit.csv\")\n",
    "crdm_params_df            = pd.read_csv(crdm_params_file) ## crdm params\n",
    "estimate_df               = pd.DataFrame(np.array(all_sub_params), columns = column_names) ## conversion of complete param estimates to dataframe\n",
    "estimate_df[\"subID\"]      = estimate_df[\"subID\"].apply('=\"{}\"'.format) ## force zero padding to final output csv\n",
    "estimate_df[\"CRDM Alpha\"] = crdm_params_df[\"Alpha\"] ## add CRDM alpha to final CASANDRE output\n",
    "estimate_df.to_csv(join(save_proj_dir, \"ric3-cpdm_best-params-est.csv\"), index = False) ## save completed dataframe w/ CRDM alpha"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
