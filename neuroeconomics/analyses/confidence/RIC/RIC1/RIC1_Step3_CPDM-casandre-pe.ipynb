{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RIC1 CPDM CASANDRE Model Fit (Parameter Estimation)**\n",
    "\n",
    "### Combines organization of confidence data, fitting of CASANDRE model, and saving output into single script.\n",
    "- Step 1. Import and structure data for CASANDRE model (previously sortTurkData.m)\n",
    "- Step 2. Provide structured data to CASANDRE model (previously getLlhChoice_Likelihood.m & fitTurkDataComb.m)\n",
    "- Step 3. Save and visualize output of CASANDRE model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information about CASANDRE\n",
    "\n",
    "- decision variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===================\n",
    "Mandy Renfro (2024)\n",
    "===================\n",
    "\"\"\"\n",
    "\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import exp, linspace, log, sqrt, sum\n",
    "np.seterr(all = \"ignore\")\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import erfcinv\n",
    "#logninv = stats.lognorm.ppf\n",
    "normcdf = stats.norm.cdf\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"white\", palette=\"muted\")\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "base_proj_dir = \"Z:/data/RIC\" ## base project directory\n",
    "data_dir      = \"Z:/data/RIC/sourcedata/RIC1\" ## directory containing data\n",
    "save_proj_dir = os.path.join(base_proj_dir, \"derivatives/RIC1/parameter_estimation/cpdm\") ## output directory\n",
    "\n",
    "## CASANDRE parameters\n",
    "cruns         = 25   ## times CASANDRE runs per Ss\n",
    "sample_rate   = 100  ## higher values produce slower, more precise estimates\n",
    "delta         = 5    ## standard deviations below and above mean, computes confidence variable distributions\n",
    "noise_sens    = 1    ## If sensory noise is set to 1, distributions of decision variable and confidence variable can be compared directly\n",
    "\n",
    "## CASANDRE options\n",
    "options       = dict(maxiter = 250, disp = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casandre_fit(guess_rate, stim_sens, stim_crit,  meta_uncert, conf_crit, stim_vals):\n",
    "    \"\"\" Compute llh of each response alternative (4).\n",
    "        Step 1 - sample decision variable denominator in steps of constant cumulative density.\n",
    "        Step 2 - compute choice distribution under each scaled sensory distribution.\n",
    "        Step 3 - average across all scaled sensory distributions to get likelihood functions.\n",
    "        **Linear transformation of normal variable is itself normal variable.\n",
    "        **Inverse of denominator used here to work with products instead of ratios.\n",
    "        INPUT\n",
    "        - params: CASANDRE parameters [guessRate, stimSens, stimCrit, uncMeta, confCrit]\n",
    "        - stim_vals: stimulus conditions in units of stimulus magnitude\n",
    "        OUTPUT\n",
    "        - choice_ll: likelihood of each choice (2 x # confidence levels x N stim_vals) \n",
    "    \"\"\"\n",
    "    sens_mean  = stim_vals * stim_sens\n",
    "    sens_crit  = stim_crit * stim_sens\n",
    "    x          = [-conf_crit, 0, conf_crit]\n",
    "    choice_llh = np.zeros((len(x) + 1, len(stim_vals)))\n",
    "    for stim_idx, curr_stim in enumerate(stim_vals):\n",
    "        mu_log_n    = log((noise_sens**2) / sqrt(meta_uncert**2 + noise_sens**2))\n",
    "        sigma_log_n = sqrt(log((meta_uncert**2) / (noise_sens**2) + 1))\n",
    "        dv_Den_x    = logninv(linspace(0.5 / sample_rate, 1 - (0.5 / sample_rate), sample_rate), mu_log_n, sigma_log_n)\n",
    "        mu          = (1 / dv_Den_x.reshape(-1, 1)) * (sens_mean[stim_idx] - sens_crit)\n",
    "        sigma       = (1 / dv_Den_x.reshape(-1, 1)) * noise_sens\n",
    "        p           = normcdf(  repeat_matrix_function_2D(x, sample_rate, 1), \n",
    "                                repeat_matrix_function_2D(mu, 1, len(x)), \n",
    "                                repeat_matrix_function_2D(np.abs(sigma), 1, len(x)))\n",
    "        ratio_dist_p = np.mean(p, axis = 0)\n",
    "        choice_llh[1:-1, stim_idx] = (guess_rate / (len(x) + 1)) + (1 - guess_rate) * (ratio_dist_p[1:] - ratio_dist_p[:-1])\n",
    "        choice_llh[   0, stim_idx] = (guess_rate / (len(x) + 1)) + (1 - guess_rate) * (ratio_dist_p[0])\n",
    "        choice_llh[  -1, stim_idx] = (guess_rate / (len(x) + 1)) + (1 - guess_rate) * (1 - ratio_dist_p[-1])\n",
    "    return choice_llh\n",
    "\n",
    "\n",
    "def _logninv(p): ## only for mu = 0 and sigma = 1\n",
    "    return exp(-sqrt(2) * erfcinv(2 * p))\n",
    "\n",
    "\n",
    "def logninv(p, mu, sigma): ## not only for mu = 0 and sigma = 1\n",
    "    ## log(logninv(p, mu, sigma)) == mu + sigma * log(logninv(p, 0, 1))\n",
    "    ## exp(log(logninv(p, mu, sigma))) == logninv(p, mu, sigma)\n",
    "    return exp(mu + sigma * log(_logninv(p)))\n",
    "\n",
    "\n",
    "def neg_ll(params_vec, stim_vals, choices, all_run_indices, all_contrast_indices, neg_llh_size):\n",
    "    \"\"\" Returns negative log likelihood for entire parameter vector.\n",
    "        INPUT\n",
    "        - param_vec:\n",
    "        - stim_vals:\n",
    "        - choices:\n",
    "        - all_run_indices:\n",
    "        - all_contrast_indices:\n",
    "        - neg_llh_size:\n",
    "        OUTPUT\n",
    "        - np.nansum(neg_llh): negative log likelihood \n",
    "    \"\"\"\n",
    "    neg_llh = np.zeros(neg_llh_size)\n",
    "    counter = 0\n",
    "    max_contrast_len = 0\n",
    "    for key in all_contrast_indices:\n",
    "        max_contrast_len = max(max_contrast_len, len(all_contrast_indices[key]))\n",
    "    for idx_run in range(len(all_run_indices)):\n",
    "        run_stim_vals = stim_vals[all_run_indices[idx_run]]\n",
    "        run_choices   = choices[all_run_indices[idx_run]]\n",
    "        for idx_contrast in range(len(all_contrast_indices[idx_run])):\n",
    "            guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit = params_vec[(np.array([0, \n",
    "                                                                                            1 + idx_contrast,\n",
    "                                                                                            1 + max_contrast_len + idx_contrast,  \n",
    "                                                                                            1 + max_contrast_len + max_contrast_len + idx_run, \n",
    "                                                                                            1 + max_contrast_len + max_contrast_len + len(all_run_indices) + idx_run]))]\n",
    "            unique_stim_choices = run_choices[all_contrast_indices[idx_run][idx_contrast]]\n",
    "            choice_llh          = casandre_fit(guess_rate, stim_sens, stim_crit, meta_uncert, conf_crit, \n",
    "                                                run_stim_vals[all_contrast_indices[idx_run][idx_contrast]])\n",
    "            neg_llh[counter]    = -sum(unique_stim_choices * log(choice_llh.T))\n",
    "            counter += 1\n",
    "    return np.nansum(neg_llh)\n",
    "\n",
    "\n",
    "def random_bounded(bounds):\n",
    "    \"\"\" Generate random value within specificed parameter range.\n",
    "        INPUT\n",
    "        - bounds: a tuple containing minimum and maximum values of the parameter range.\n",
    "        OUTPUT\n",
    "        - random value within the bounded range.\n",
    "    \"\"\"\n",
    "    return np.random.random() * (bounds[1] - bounds[0]) + bounds[0]\n",
    "\n",
    "\n",
    "def repeat_matrix_function_2D(arr, x, y = 1):\n",
    "    \"\"\" Python function to replicate MATLAB's repmat()\n",
    "        INPUT\n",
    "        - arr: array to be copied\n",
    "        - x: number of rows\n",
    "        - y: number of columns\n",
    "        OUTPUT\n",
    "        - new_arr: new array \n",
    "    \"\"\"\n",
    "    new_arr = arr.copy()\n",
    "    for i in range(1, x):\n",
    "        new_arr = np.vstack((new_arr, arr))\n",
    "    arr_h = new_arr.copy()\n",
    "    for i in range(1, y):\n",
    "        new_arr = np.hstack((new_arr, arr_h))\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_rate_bounds  = (   0, 0.075)\n",
    "stim_sens_bounds   = (0.01,    10)\n",
    "stim_crit_bounds   = (  -3,     3)\n",
    "meta_uncert_bounds = ( 0.1,     3)\n",
    "conf_crit_bounds   = (0.01,   5.1)\n",
    "\n",
    "subs = [] ## all subject IDs\n",
    "all_sub_params = []\n",
    "\n",
    "files = sorted(glob(os.path.join(data_dir, \"23_IDM_*.csv\"))) ## grab all participant datafiles\n",
    "for curr_file in files: ## iterate through globbed files and save subject ID to a list\n",
    "    sub_id = os.path.basename(curr_file)[7:11] ## grab first 5 indices of filename string\n",
    "    if not sub_id in subs: ## check if already in list\n",
    "        subs.append(sub_id) ## if not, append new Ss ID to list\n",
    "\n",
    "skip_num = 0\n",
    "if os.path.exists(join(save_proj_dir, \"ric1-cpdm_best-params-est.csv\")): ## new Ss\n",
    "    existing_df = pd.read_csv(join(save_proj_dir, \"ric1-cpdm_best-params-est.csv\")) ## make new Ss save directory\n",
    "    existing_df[\"subID\"] = [val[2:-1] for val in existing_df[\"subID\"]]\n",
    "    skip_num = len(existing_df[\"subID\"].values)\n",
    "    all_sub_params = existing_df.values.tolist()\n",
    "\n",
    "for idx, sub in enumerate(subs): ## iterate through Ss ID list\n",
    "    if idx + 1 <= skip_num:\n",
    "        continue\n",
    "    sub_files = sorted(glob(os.path.join(data_dir, \"23_IDM_{0}.csv\".format(sub)))) ## grab all IDM task csvs\n",
    "    sub_cols  = [\"run_dims\", \"orient\", \"contrast\", \"acc\", \"conf\", \"choice\"] ## trial/Ss resp elements\n",
    "    sub_df    = pd.DataFrame(columns = sub_cols) ## subject-specific dataframe w/ preset columns\n",
    "    raw_df    = pd.read_csv(sub_files[0]) ## open current data file\n",
    "    df        = raw_df.loc[(raw_df[\"cpdm_trial_type\"] == \"task\") & (raw_df[\"cpdm_trial_resp.keys\"].notnull())] ## only CPDM task trials w/ responses\n",
    "    sub_df[\"run_dims\"] = df[\"cpdm_run_dimension\"] ## dimensions for current trial (volatility/risk levels = number/difficulty of orientations/contrasts)\n",
    "    sub_df[\"orient\"]   = df[\"cpdm_gabor_orient\"] ## orientation of gabor patch\n",
    "    sub_df[\"contrast\"] = df[\"cpdm_gabor_contrast\"] ## contrast of gabor patch\n",
    "    sub_df[\"acc\"]      = df[\"cpdm_acc\"] ## Ss accuracy\n",
    "    sub_df[\"conf\"]     = df[\"cpdm_conf\"] ## Ss confidence in perceptual accuracy\n",
    "    sub_df[\"choice\"]   = df[\"cpdm_trial_resp.keys\"] ## trial choice (q=high conf,left tilt; a=low conf,left tilt; p=high conf,right tilt; l=low conf,right tilt)\n",
    "    choice_dict        = {'q':-2, 'a':-1, 'l':1, 'p':2} \n",
    "    sub_df[\"choice\"]   = df[\"cpdm_trial_resp.keys\"].replace(choice_dict) ## recode CPDM Ss choice responses\n",
    "    \n",
    "    stim_vals     = sub_df[\"orient\"].values\n",
    "    contrast_vals = sub_df[\"contrast\"].values\n",
    "    choices       = sub_df[\"choice\"].values\n",
    "    math_choices  = np.zeros((len(choices), len(choice_dict.keys())))\n",
    "    math_choices[np.where(choices ==  2)] = np.array([0, 0, 0, 1])\n",
    "    math_choices[np.where(choices ==  1)] = np.array([0, 0, 1, 0])\n",
    "    math_choices[np.where(choices == -1)] = np.array([0, 1, 0, 0])\n",
    "    math_choices[np.where(choices == -2)] = np.array([1, 0, 0, 0])\n",
    "    \n",
    "    all_run_indices      = []\n",
    "    all_contrast_indices = {}\n",
    "    nll_counter          = 0\n",
    "    for idx_run, run_label in enumerate(sorted(sub_df[\"run_dims\"].unique())): ## go through each task run labels\n",
    "        trial_indices = np.where(sub_df[\"run_dims\"].values == run_label)\n",
    "        all_run_indices.append(trial_indices)\n",
    "        run_contrast_vals = contrast_vals[trial_indices]\n",
    "        all_contrast_indices[idx_run] = []\n",
    "        for unique_contrast in sorted(np.unique(run_contrast_vals)): ## go through each of the unique stimulus contrasts\n",
    "            unique_contrast_indices = np.where(run_contrast_vals == unique_contrast)\n",
    "            all_contrast_indices[idx_run].append(unique_contrast_indices)\n",
    "            nll_counter += len(unique_contrast_indices)\n",
    "\n",
    "    best_param_est = None\n",
    "    best_nll       = None\n",
    "    print(sub, end = \"\\r\")\n",
    "    run_labels     = dict(high_vol_high_risk = \"HVHR\", high_vol_low_risk = \"HVLR\", low_vol_high_risk = \"LVHR\", low_vol_low_risk = \"LVLR\")\n",
    "    for crun in range(cruns):\n",
    "        search_start = [random_bounded(guess_rate_bounds)]\n",
    "        bounds       = [guess_rate_bounds]\n",
    "        column_names = [\"subID\", \"Guess Rate\"]\n",
    "        for curr_contrast in sorted(np.unique(contrast_vals)):\n",
    "            search_start.append(random_bounded(stim_sens_bounds))\n",
    "            bounds.append(stim_sens_bounds)\n",
    "            column_names.append(\"Stimulus Sensitivity {}\".format(curr_contrast))\n",
    "        for curr_contrast in sorted(np.unique(contrast_vals)):\n",
    "            search_start.append(random_bounded(stim_crit_bounds))\n",
    "            bounds.append(stim_crit_bounds)\n",
    "            column_names.append(\"Stimulus Criterion {}\".format(curr_contrast))\n",
    "        for curr_run in sorted(sub_df[\"run_dims\"].unique()):\n",
    "            search_start.append(random_bounded(meta_uncert_bounds))\n",
    "            bounds.append(meta_uncert_bounds)\n",
    "            column_names.append(\"Meta Uncertainty {}\".format(run_labels[curr_run]))\n",
    "        for curr_run in sorted(sub_df[\"run_dims\"].unique()):\n",
    "            search_start.append(random_bounded(conf_crit_bounds))\n",
    "            bounds.append(conf_crit_bounds)\n",
    "            column_names.append(\"Confidence Criterion {}\".format(run_labels[curr_run]))\n",
    "        search_start = np.array(search_start)\n",
    "        column_names.append(\"Best NLL\")\n",
    "        estimate = minimize(neg_ll, \n",
    "                            x0      = search_start, \n",
    "                            method  = \"L-BFGS-B\", \n",
    "                            bounds  = bounds,\n",
    "                            args    = (stim_vals, math_choices, all_run_indices, all_contrast_indices, nll_counter), \n",
    "                            options = options)\n",
    "        if best_nll is None or best_nll > estimate.fun:\n",
    "            best_nll       = estimate.fun\n",
    "            best_param_est = np.hstack((sub, estimate.x, best_nll))\n",
    "    all_sub_params.append(best_param_est)\n",
    "    estimate_df = pd.DataFrame(np.array(all_sub_params), columns = column_names)\n",
    "    estimate_df[\"subID\"] = estimate_df[\"subID\"].apply('=\"{}\"'.format)\n",
    "    estimate_df.to_csv(join(save_proj_dir, \"ric1-cpdm_best-params-est.csv\"), index = False)\n",
    "\n",
    "estimate_df = pd.DataFrame(np.array(all_sub_params), columns = column_names)\n",
    "estimate_df[\"subID\"] = estimate_df[\"subID\"].apply('=\"{}\"'.format)\n",
    "estimate_df.to_csv(join(save_proj_dir, \"ric1-cpdm_best-params-est.csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
