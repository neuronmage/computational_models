{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDM_Masterlist\n",
    "## What does it do?\n",
    "#### - Creates a masterfile overview of completed participants and provides payment dates. Also creates simplified csv files, removing unused columns.\n",
    "## Before you run this code the first time:\n",
    "#### Step 1. Create a new folder on your Desktop called \"idm_files\". Save this notebook in that folder. This folder will contain everything relating to this script.\n",
    "#### Step 2. Download the zipped data file from Pavlovia.\n",
    "#### Step 3. Extract the \"data\" folder from the compressed zip. Move unzipped \"data\" folder to idm_files. \n",
    "#### Step 4. Run the cells of this notebook. \n",
    "## Input\n",
    "#### Extracted Pavlovia data files\n",
    "## Output\n",
    "#### idm_masterlist.csv\n",
    "##### - Contains assigned IDM ID#, MTurk WorkerID, date/time of data collection, bonus task, bonus amount, bonus delay, each task response rate, completion code, and Ss payment date\n",
    "#### 23_IDM_####.csv\n",
    "##### - Simplified csv data file for each completed Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "\n",
    "#regular expressions to identify different date patterns (web browsers differences)\n",
    "date_type1 = re.compile(r\"[0-9]{4}-[0-9]{2}-[0-9]{2}\", re.IGNORECASE) #2023-03-27\n",
    "date_type2 = re.compile(r\"[0-9]{2}-[0-9]{2}-[0-9]{4}\", re.IGNORECASE) #03-27-2023\n",
    "date_type3 = re.compile(r\"[0-9]{2}/[0-9]{2}/[0-9]{4}\", re.IGNORECASE) #03/27/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23_IDM_0001\n",
      "23_IDM_0002\n",
      "23_IDM_0003\n",
      "23_IDM_0004\n",
      "23_IDM_0005\n",
      "23_IDM_0006\n",
      "23_IDM_0007\n",
      "23_IDM_0008\n",
      "23_IDM_0009\n",
      "23_IDM_0010\n",
      "23_IDM_0011\n",
      "23_IDM_0012\n",
      "23_IDM_0013\n",
      "23_IDM_0014\n",
      "23_IDM_0015\n",
      "23_IDM_0016\n",
      "23_IDM_0017\n",
      "23_IDM_0018\n",
      "23_IDM_0019\n",
      "23_IDM_0020\n",
      "23_IDM_0021\n",
      "23_IDM_0022\n",
      "23_IDM_0023\n",
      "23_IDM_0024\n",
      "23_IDM_0025\n",
      "23_IDM_0026\n",
      "23_IDM_0027\n",
      "23_IDM_0028\n",
      "23_IDM_0029\n",
      "23_IDM_0030\n",
      "23_IDM_0031\n",
      "23_IDM_0032\n",
      "23_IDM_0033\n",
      "23_IDM_0034\n",
      "23_IDM_0035\n",
      "23_IDM_0036\n",
      "23_IDM_0037\n",
      "23_IDM_0038\n",
      "23_IDM_0039\n",
      "23_IDM_0040\n",
      "23_IDM_0041\n",
      "23_IDM_0042\n",
      "23_IDM_0043\n",
      "23_IDM_0044\n",
      "23_IDM_0045\n",
      "23_IDM_0046\n",
      "23_IDM_0047\n",
      "23_IDM_0048\n",
      "23_IDM_0049\n",
      "23_IDM_0050\n",
      "23_IDM_0051\n",
      "23_IDM_0052\n",
      "23_IDM_0053\n",
      "23_IDM_0054\n",
      "23_IDM_0055\n",
      "23_IDM_0056\n",
      "23_IDM_0057\n",
      "23_IDM_0058\n",
      "23_IDM_0059\n",
      "23_IDM_0060\n",
      "23_IDM_0061\n",
      "23_IDM_0062\n",
      "23_IDM_0063\n",
      "23_IDM_0064\n",
      "23_IDM_0065\n",
      "23_IDM_0066\n",
      "23_IDM_0067\n",
      "23_IDM_0068\n",
      "23_IDM_0069\n",
      "23_IDM_0070\n",
      "23_IDM_0071\n",
      "23_IDM_0072\n",
      "23_IDM_0073\n",
      "23_IDM_0074\n",
      "23_IDM_0075\n",
      "23_IDM_0076\n",
      "23_IDM_0077\n",
      "23_IDM_0078\n",
      "23_IDM_0079\n",
      "23_IDM_0080\n",
      "23_IDM_0081\n",
      "23_IDM_0082\n",
      "23_IDM_0083\n",
      "23_IDM_0084\n",
      "23_IDM_0085\n",
      "23_IDM_0086\n",
      "23_IDM_0087\n",
      "23_IDM_0088\n",
      "23_IDM_0089\n",
      "23_IDM_0090\n",
      "23_IDM_0091\n",
      "23_IDM_0092\n",
      "23_IDM_0093\n",
      "23_IDM_0094\n",
      "23_IDM_0095\n",
      "23_IDM_0096\n",
      "23_IDM_0097\n",
      "23_IDM_0098\n",
      "23_IDM_0099\n",
      "23_IDM_0100\n",
      "23_IDM_0101\n",
      "23_IDM_0102\n",
      "23_IDM_0103\n",
      "23_IDM_0104\n",
      "23_IDM_0105\n",
      "23_IDM_0106\n",
      "23_IDM_0107\n",
      "23_IDM_0108\n",
      "23_IDM_0109\n",
      "23_IDM_0110\n",
      "23_IDM_0111\n",
      "23_IDM_0112\n"
     ]
    }
   ],
   "source": [
    "def check_empty(filename): #determines if data file is empty \n",
    "    lines = \"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines += line\n",
    "    return len(lines) < 10\n",
    "\n",
    "filepath = os.getcwd() #relative path - current working directory\n",
    "\n",
    "if os.path.exists(join(filepath, \"IDM_simp_files\")): #checks to see if simp folder exists\n",
    "    shutil.rmtree(join(filepath, \"IDM_simp_files\")) #removes old simp folder and files\n",
    "    os.makedirs(join(filepath, \"IDM_simp_files\")) #creates new simp folder\n",
    "else: #no previous simp folder\n",
    "    os.makedirs(join(filepath, \"IDM_simp_files\")) #creates new simp folder\n",
    "\n",
    "data = {\"subID\": [], \"workerID\": [], \"DateTime\": [], \"CompCode\": [],  \"CPDM\": [], \"CRDM\": [], \"CDD\":[],  \n",
    "        \"BonusTask\": [], \"BonusDelay\": [], \"BONUS_AMT\": [], \"PAY_DATE\":[]} #dictionary containing all data to be saved to IDM masterfile\n",
    "date_idx = [] #index of date for completed files\n",
    "date = [] #17 digit number representing date and time Ss file was created (\"date\")\n",
    "date_format = [] #standardized date/time format(2023-01-25_15h10.30.241)\n",
    "\n",
    "pavlovia_files = sorted(glob(join(filepath, \"data/*.csv\"))) #original Pavlovia csv files\n",
    "##get date/time for each complete Ss data file, standardize date/time format, and convert date/time to integer##\n",
    "for i, data_name in enumerate(pavlovia_files): \n",
    "    if check_empty(data_name) is False: #files which are not completely empty\n",
    "        pav_data_df = pd.read_csv(data_name, on_bad_lines='skip') #open csv as Pandas dataframe\n",
    "        if \"completion_code\" in list(pav_data_df.keys()): #completed experimental runs\n",
    "            date_idx.append(i) #save index of current file within all Pavlovia files\n",
    "            date_fix1 = pav_data_df[\"date\"][0].split(\"_\") #split first value in \"date\" column [date and time]\n",
    "            if date_type1.match(pav_data_df[\"date\"][0]): ##date/time formatted as 2023-01-25_15h10.30.241\n",
    "                date_format.append(pav_data_df[\"date\"][0]) #save datetime to data dict\n",
    "                a = pav_data_df[\"date\"][0].replace(\"h\", \".\").replace(\"_\", \".\").replace(\"-\",\".\").split(\".\") #replace other separators with, then splits on, \".\"\n",
    "                date.append(\"\".join([str(item) for item in a])) #save concatenated .split() output\n",
    "            elif date_type2.match(pav_data_df[\"date\"][0]): ##date/time format as 02-23-2023_15h10.25.875             \n",
    "                date_fix2 = date_fix1[0].split(\"-\") #split month, day, year and reassemble to match standard format\n",
    "                date_fix3 = date_fix2[2] + \"-\" + date_fix2[0] + \"-\" + date_fix2[1] + \"_\" + date_fix1[1] #format to standard date/time\n",
    "                date_format.append(date_fix3) #save standard date/time to list\n",
    "                a = date_fix3.replace(\"h\", \".\").replace(\"_\", \".\").replace(\"-\",\".\").split(\".\") #replace other separators with, then splits on, \".\"\n",
    "                date.append(\"\".join([str(item) for item in a])) #save concatenated .split() output                      \n",
    "            else: ##date/time format as 03/27/2023_05h14.47.829\n",
    "                date_fix2 = date_fix1[0].split(\"/\") #split month, day, year and reassemble to match standard format\n",
    "                date_fix3 = date_fix2[2] + \"-\" + date_fix2[0] + \"-\" + date_fix2[1] + \"_\" + date_fix1[1] #reformats to standard date/time\n",
    "                date_format.append(date_fix3) #save standard date/time to list\n",
    "                a = date_fix3.replace(\"h\", \".\").replace(\"_\", \".\").replace(\"-\",\".\").split(\".\") #replace other separators with, then splits on, \".\"\n",
    "                date.append(\"\".join([str(item) for item in a])) #save concatenated .split() output\n",
    "\n",
    "date_idx = np.array(date_idx) #convert to Numpy array to use .argsort()\n",
    "date = np.array(date) #convert to Numpy array to use .argsort()\n",
    "sorted_idx = date_idx[date.argsort()] #sort date_idx by date\n",
    "sorted_date_format = np.array(date_format)[date.argsort()] #sort date_format by date\n",
    "\n",
    "for i, good_idx in enumerate(sorted_idx): #iterate through each of the completed Ss data files\n",
    "    data_df = pd.read_csv(pavlovia_files[good_idx]) #open csv file as Pandas dataframe\n",
    "    data[\"subID\"].append(str(sorted_date_format[i][2:4])+\"_IDM_\"+f\"{i+1:04}\") #records new IDM ID#\n",
    "    data[\"workerID\"].append(data_df[\"workerId\"][0]) #records MTurk ID#   \n",
    "    data['DateTime'].append(sorted_date_format[i]) #records datetime   \n",
    "    data[\"BonusTask\"].append(data_df[\"idm_bonus_exp\"][1039]) #records bonus task\n",
    "    data[\"CompCode\"].append(int(data_df[\"completion_code\"][1043])) #records completion code\n",
    "    if \"cpdm_bonus_amt\" in list(data_df.keys()): #CPDM Bonus Trial\n",
    "        data[\"BONUS_AMT\"].append(int(data_df[\"cpdm_bonus_amt\"][1040])) #records CPDM bonus amount\n",
    "        data[\"BonusDelay\"].append(0) #records CPDM bonus delay as 0\n",
    "        data[\"PAY_DATE\"].append(datetime.datetime(*map(int,data[\"DateTime\"][i][:10].split(\"-\")))) #no delay, gets current date (month/day/year)\n",
    "    elif \"crdm_bonus_amt\" in list(data_df.keys()): #CRDM Bonus Trial\n",
    "        data[\"BONUS_AMT\"].append(int(data_df[\"crdm_bonus_amt\"][1041])) #records CRDM bonus amount\n",
    "        data[\"BonusDelay\"].append(0) #records CRDM bonus delay as 0\n",
    "        data[\"PAY_DATE\"].append(datetime.datetime(*map(int,data[\"DateTime\"][i][:10].split(\"-\")))) #no delay, gets current date (month/day/year)\n",
    "    elif \"cdd_bonus_amt\" in list(data_df.keys()): #CDD Bonus Trial (correct key)\n",
    "        data[\"BONUS_AMT\"].append(int(data_df[\"cdd_bonus_amt\"][1042])) #records CDD bonus amount\n",
    "        data[\"BonusDelay\"].append(int(data_df[\"cdd_bonus_delay\"][1042])) #records CDD bonus delay\n",
    "        #calculates payment date from current date plus number of delay days\n",
    "        data[\"PAY_DATE\"].append(datetime.datetime(*map(int,data[\"DateTime\"][i][:10].split(\"-\")))+datetime.timedelta(days=int(data_df[\"cdd_bonus_delay\"][1042])))\n",
    "    elif \"cdd_bonus_amount\" in list(data_df.keys()): #CDD Bonus Trial (typo key)\n",
    "        data[\"BONUS_AMT\"].append(int(data_df[\"cdd_bonus_amount\"][1042])) #records CDD bonus amount\n",
    "        data[\"BonusDelay\"].append(int(data_df[\"cdd_bonus_delay\"][1042])) #records CDD bonus delay\n",
    "        #calculates payment date from current date plus number of delay days\n",
    "        data[\"PAY_DATE\"].append(datetime.datetime(*map(int,data[\"DateTime\"][i][:10].split(\"-\")))+datetime.timedelta(days=int(data_df[\"cdd_bonus_delay\"][1042])))\n",
    "    #calculates CPDM proportion of response trials     \n",
    "    data[\"CPDM\"].append(round(len(np.where((data_df[\"cpdm_trial_type\"].values == \"task\") & ((data_df[\"cpdm_trial_resp.keys\"].values == \"q\") | (data_df[\"cpdm_trial_resp.keys\"].values == \"p\") \n",
    "    | (data_df[\"cpdm_trial_resp.keys\"].values == \"a\") | (data_df[\"cpdm_trial_resp.keys\"].values == \"l\")))[0])/800, 2))\n",
    "    #calculates CRDM proportion of response trials \n",
    "    data[\"CRDM\"].append(round(len(np.where((data_df[\"crdm_trial_type\"].values == \"task\") & ((data_df[\"crdm_trial_resp.keys\"].values == 1) | (data_df[\"crdm_trial_resp.keys\"].values == 2)))[0])/80, 2))\n",
    "    #calculates CDD proportion of response trials       \n",
    "    data[\"CDD\"].append(round(len(np.where((data_df[\"cdd_trial_type\"].values == \"task\") & ((data_df[\"cdd_trial_resp.keys\"].values == 1) | (data_df[\"cdd_trial_resp.keys\"].values == 2)))[0])/96, 2))\n",
    "\n",
    "    ##drop unused columns to create simplied CSV##\n",
    "    simp_df = data_df.drop(columns=[\"participant\", \"workerId\", \"hitId\", \"assignmentId\", \"a\", \"tp_a\", \"b\", \"tp_b\", \"c\", \"tp_c\", \"survey1.thisRepN\", \n",
    "                                    \"survey1.thisTrialN\", \"survey1.thisN\", \"survey1.thisIndex\", \"survey1.ran\", \n",
    "                                    \"resps\", \"resp1\", \"resp2\", \"resp3\", \"resp4\", \"resp5\", \"resp6\",\n",
    "                                    \"survey2.thisRepN\", \"survey2.thisTrialN\", \"survey2.thisN\", \"survey2.thisIndex\", \"survey2.ran\",\n",
    "                                    \"crdm_pract_trial_resp.keys\", \"crdm_pract_trial_resp.corr\", \n",
    "                                    \"cdd_pract_trial_resp.keys\", \"cdd_pract_trial_resp.corr\",\"cpdm_pract_trial_resp.keys\"])\n",
    "    #because some Ss did not respond to specific task practice trials, removing each practice.rt requires its own conditional statement\n",
    "    if \"crdm_pract_trial_resp.rt\" in simp_df.keys(): \n",
    "        simp_df = simp_df.drop(columns=[\"crdm_pract_trial_resp.rt\"])\n",
    "    if \"cdd_pract_trial_resp.rt\" in simp_df.keys():\n",
    "        simp_df = simp_df.drop(columns=[\"cdd_pract_trial_resp.rt\"])\n",
    "    if \"cpdm_pract_trial_resp.rt\" in simp_df.keys():\n",
    "        simp_df = simp_df.drop(columns=[\"cpdm_pract_trial_resp.rt\"])\n",
    "    #setup to remove unnecessary Psychopy columns from output\n",
    "    for key in [\"crdm_pract_trials1.thisRepN\", \"crdm_pract_trials2.thisRepN\", \"cdd_pract_trials1.thisRepN\", \"cdd_pract_trials2.thisRepN\",\n",
    "                \"cpdm_pract_trials1.thisRepN\", \"cpdm_pract_trials2.thisRepN\", \"cpdm_pract_trials3.thisRepN\"]:\n",
    "        task_name = key.split(\"_\")[0] #gets just the task name\n",
    "        task_num = key[-10] #gets just the loop number\n",
    "        #removes Psychopy-specific CPDM condition run columns\n",
    "        if key in data_df.keys() and key in [\"cpdm_pract_trials1.thisRepN\", \"cpdm_pract_trials2.thisRepN\", \"cpdm_pract_trials3.thisRepN\"]:    \n",
    "            simp_df = simp_df.drop(columns=[\"condition_runs{0}.thisRepN\".format(task_num), \"condition_runs{0}.thisTrialN\".format(task_num),\n",
    "                                            \"condition_runs{0}.thisN\".format(task_num), \"condition_runs{0}.thisIndex\".format(task_num), \n",
    "                                            \"condition_runs{0}.ran\".format(task_num)])\n",
    "        #removes Psychopy-specific columns for each loop of all three tasks \n",
    "        if key in list(simp_df.keys()):\n",
    "            simp_df = simp_df.drop(columns=[\"{0}_pract_trials{1}.thisRepN\".format(task_name,task_num), \"{0}_pract_trials{1}.thisTrialN\".format(task_name,task_num), #practice\n",
    "                                            \"{0}_pract_trials{1}.thisN\".format(task_name,task_num), \"{0}_pract_trials{1}.thisIndex\".format(task_name,task_num),\n",
    "                                            \"{0}_pract_trials{1}.ran\".format(task_name,task_num), \n",
    "                                            \"{0}_trials{1}.thisRepN\".format(task_name,task_num), \"{0}_trials{1}.thisN\".format(task_name,task_num), #task \n",
    "                                            \"{0}_trials{1}.ran\".format(task_name,task_num)]) \n",
    "    simp_df.to_csv(\"IDM_simp_files/\"+ str(sorted_date_format[i][2:4])+\"_IDM_\"+f\"{i+1:04}\"+\".csv\") #save simplied data file to csv and label with IDM ID#\n",
    "    print(str(sorted_date_format[i][2:4])+\"_IDM_\"+f\"{i+1:04}\") #print new task-specific subject ID#\n",
    "                    \n",
    "masterlist_df = pd.DataFrame(data) #convert dictionary to Pandas DF\n",
    "masterlist_df.to_csv(\"IDM_masterlist.csv\") #save masterlist DF to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:56:26\n"
     ]
    }
   ],
   "source": [
    "import datetime, time\n",
    " \n",
    "secs = 6986\n",
    " \n",
    "result = datetime.timedelta(seconds = secs)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing around with datetime.datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-16_05h14.47.829\n",
      "['2023', '02', '16', '05', '14', '47', '829']\n",
      "20230216051447829\n"
     ]
    }
   ],
   "source": [
    "print(data[\"DateTime\"][0])\n",
    "time1 = data[\"DateTime\"][0].replace(\"h\", \".\").replace(\"_\", \".\").replace(\"-\",\".\").split(\".\")\n",
    "print(time1)\n",
    "date = \"\".join([str(item) for item in time1])\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023', '01', '25']\n",
      "2023-01-25 00:00:00\n",
      "2023-01-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "date1 = masterlist_df[\"datetime\"].values[0][:10].split(\"-\")\n",
    "print(date1)\n",
    "x = datetime.datetime(*map(int, date1))\n",
    "print(x)\n",
    "print(x+datetime.timedelta(days=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
